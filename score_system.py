# -*- coding: utf-8 -*-
"""Score_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hWuXnWlDqeUzDVQDoXIeDd4BFV6pxgsd
"""

import json
import re

def validate_json_file(file_path, save_valid_as=None):
    """
    Validates and attempts to fix a malformed JSON file.
    If valid, it returns parsed data. If invalid, it tries to extract as many valid records as possible.
    Optionally saves the cleaned data.
    """
    print(f"Checking: {file_path}")

    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        print("JSON is valid and fully loaded.")
        return data

    except json.JSONDecodeError as e:
        print(f" JSON is malformed: {e}")
        print(" Attempting to recover valid records...")

        with open(file_path, 'r') as f:
            raw = f.read()


        raw = raw.strip().strip(',[]')
        entries = re.split(r'}\s*,\s*{', raw)

        valid_records = []
        for i, entry in enumerate(entries):

            if not entry.startswith('{'):
                entry = '{' + entry
            if not entry.endswith('}'):
                entry = entry + '}'

            try:
                obj = json.loads(entry)
                valid_records.append(obj)
            except json.JSONDecodeError:
                print(f" Skipping malformed object at index {i}")

        print(f" Recovered {len(valid_records)} valid records.")

        # Save cleaned version
        if save_valid_as:
            with open(save_valid_as, 'w') as out:
                json.dump(valid_records, out, indent=2)
            print(f" Cleaned file saved as: {save_valid_as}")

        return valid_records


if __name__ == "__main__":
    cleaned_data = validate_json_file("user-wallet-transactions.json", save_valid_as="cleaned_transactions.json")

import pandas as pd
import json

with open('cleaned_transactions.json', 'r') as f:
    data = json.load(f)

len(data)

print(data[0:2])

import pprint
pprint.pprint(data[0:2])

#check the length of all dictionries
from collections import Counter
Counter([len(items) for items in data])

df = pd.DataFrame(data)

df.columns

df.describe()

df.isnull().sum()

wallet_unique = df['userWallet'].nunique()
print(wallet_unique)

wallet_counts = df['userWallet'].value_counts()

one_tx_wallets = wallet_counts[wallet_counts == 1].index.tolist()
many_tx_wallets = wallet_counts[wallet_counts > 1].index.tolist()
no_tx =wallet_counts[wallet_counts == 0].index.tolist()

print("Transaction Categories:")
print(f"One transaction only: {len(one_tx_wallets)} wallets")
print(f"Many transactions:{len(many_tx_wallets)} wallets")
print(f"No transactions: {len(no_tx)} wallets")

one_tx_df = df[df['userWallet'].isin(one_tx_wallets)]
action_counts = one_tx_df['action'].value_counts()
print("Actions performed by one-time wallets:")
print(action_counts)

print(one_tx_df[['userWallet', 'action']])

many_tx_df = df[df['userWallet'].isin(many_tx_wallets)]
action_counts = many_tx_df['action'].value_counts()
print("Actions performed by many-time wallets:")
print(action_counts)

def calculate_usd(row):
    try:
        amount = float(row['actionData']['amount'])
        price = float(row['actionData'].get('assetPriceUSD', 1))
        # Heuristically normalize token units (many tokens use 18 decimals)
        normalized_amount = amount / 1e18
        return normalized_amount * price
    except:
        return 0

df['usd_value'] = df.apply(calculate_usd, axis=1)

df.columns

df['is_deposit'] = df['action'] == 'deposit'
df['is_borrow'] = df['action'] == 'borrow'
df['is_repay'] = df['action'] == 'repay'
df['is_redeem'] = df['action'] == 'redeemunderlying'
df['is_liquidation'] = df['action'] == 'liquidationcall'

agg_df = df.groupby('userWallet').agg(
    num_deposit=('is_deposit', 'sum'),
    num_borrow=('is_borrow', 'sum'),
    num_repay=('is_repay', 'sum'),
    num_redeem=('is_redeem', 'sum'),
    num_liquidation=('is_liquidation', 'sum'),
    total_deposit_usd=('usd_value', lambda x: x[df['is_deposit']].sum()),
    total_borrow_usd=('usd_value', lambda x: x[df['is_borrow']].sum()),
    total_repay_usd=('usd_value', lambda x: x[df['is_repay']].sum()),
    total_redeem_usd=('usd_value', lambda x: x[df['is_redeem']].sum()),
    tx_count=('action', 'count')
).reset_index()

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

X = agg_df.drop(columns=['userWallet'])


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

kmeans = KMeans(n_clusters=4, random_state=42)
agg_df['cluster'] = kmeans.fit_predict(X_scaled)

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import pandas as pd


X = agg_df.drop(columns=['userWallet'])


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


kmeans = KMeans(n_clusters=4, random_state=42)
agg_df['cluster'] = kmeans.fit_predict(X_scaled)


numeric_cols = agg_df.select_dtypes(include=['int64', 'float64']).columns
cluster_summary = agg_df.groupby('cluster')[numeric_cols].mean()
print(cluster_summary)

cluster_to_score = {
    0: 850,   # High activity and good behavior
    1: 650,   # Moderate usage
    2: 350,   # Risky / minimal usage
    3: 150    # Liquidated / exploitative
}

agg_df['credit_score'] = agg_df['cluster'].map(cluster_to_score)

agg_df[['userWallet', 'cluster', 'credit_score']].to_json(
    'wallet_scores.json', orient='records', indent=2
)

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.hist(agg_df['credit_score'], bins=[0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],
         edgecolor='black')
plt.title('Wallet Credit Score Distribution')
plt.xlabel('Score Range')
plt.ylabel('Number of Wallets')
plt.grid(True)
plt.tight_layout()
plt.savefig("score_distribution.png")
plt.show()

df.shape

print(agg_df.columns)

from sklearn.decomposition import PCA

X = agg_df.drop(columns=['userWallet', 'cluster', 'credit_score'], errors='ignore')
X_scaled = StandardScaler().fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

agg_df['pca1'] = X_pca[:, 0]
agg_df['pca2'] = X_pca[:, 1]

plt.figure(figsize=(8, 6))
sns.scatterplot(x='pca1', y='pca2', hue='cluster', data=agg_df, palette='tab10')
plt.title('Wallet Clusters (PCA Visualization)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')
plt.tight_layout()
plt.savefig("wallet_clusters_pca.png")
plt.show()

cluster_tx_counts = agg_df.groupby('cluster')[['num_deposit', 'num_borrow', 'num_repay', 'num_redeem', 'num_liquidation']].mean()

cluster_tx_counts.plot(kind='bar', figsize=(12, 6))
plt.title("Average Transaction Type Count by Cluster")
plt.xlabel("Cluster")
plt.ylabel("Average Number of Actions")
plt.xticks(rotation=0)
plt.grid(True)
plt.legend(title="Action Type")
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 8))
sns.heatmap(agg_df.drop(['userWallet', 'cluster'], axis=1).corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Aggregated Features')
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Bin the credit scores into ranges
bins = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]
labels = ['0-100', '100-200', '200-300', '300-400', '400-500',
          '500-600', '600-700', '700-800', '800-900', '900-1000']

agg_df['score_range'] = pd.cut(agg_df['credit_score'], bins=bins, labels=labels, right=True)
score_distribution = agg_df['score_range'].value_counts().sort_index()

print(score_distribution)

# Plot score distribution
plt.figure(figsize=(10, 6))
sns.barplot(x=score_distribution.index, y=score_distribution.values, palette="viridis")
plt.title("Wallet Credit Score Distribution")
plt.xlabel("Credit Score Range")
plt.ylabel("Number of Wallets")
plt.xticks(rotation=45)
plt.tight_layout()
plt.grid(True, linestyle='--', alpha=0.3)
plt.show()

